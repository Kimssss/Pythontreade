#!/usr/bin/env python3
"""
AI ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ë©”ì¸ ëª¨ë“ˆ
- DQN ê°•í™”í•™ìŠµ ê¸°ë°˜ íŠ¸ë ˆì´ë”©
- AutoML í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ìë™ ì¬í›ˆë ¨
"""

import sys
import os
import traceback
from pathlib import Path

# ìë™ íŒ¨í‚¤ì§€ ì„¤ì¹˜
try:
    from auto_install import check_and_install_requirements, auto_install_on_import
    
    # ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ëª¨ë“  ì˜ì¡´ì„± í™•ì¸
    print("ğŸ”§ ì˜ì¡´ì„± íŒ¨í‚¤ì§€ í™•ì¸ ì¤‘...")
    if not check_and_install_requirements():
        print("âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨. ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(1)
    print("âœ… ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ\n")
    
except ImportError as e:
    print(f"âš ï¸ ìë™ ì„¤ì¹˜ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("ìˆ˜ë™ìœ¼ë¡œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

# ì´ì œ ì•ˆì „í•˜ê²Œ ë‹¤ë¥¸ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸
import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime, timedelta
import json
import logging
from typing import Dict, List, Tuple, Optional
import threading
import time

# ë¡œì»¬ ëª¨ë“ˆ
from kis_api import KisAPI
from config import Config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ai_trading.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class TradingEnvironment:
    """
    ê°•í™”í•™ìŠµì„ ìœ„í•œ íŠ¸ë ˆì´ë”© í™˜ê²½ (OpenAI Gym ìŠ¤íƒ€ì¼)
    """
    
    def __init__(self, kis_api: KisAPI, stocks: List[str], initial_balance: float = 1000000):
        self.kis_api = kis_api
        self.stocks = stocks  # ê±°ë˜í•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸
        self.initial_balance = initial_balance
        self.current_balance = initial_balance
        self.positions = {stock: 0 for stock in stocks}  # ê° ì¢…ëª©ë³„ ë³´ìœ  ìˆ˜ëŸ‰
        self.prices = {stock: 0 for stock in stocks}     # ê° ì¢…ëª©ë³„ í˜„ì¬ê°€
        self.price_history = {stock: [] for stock in stocks}  # ê°€ê²© ì´ë ¥
        self.step_count = 0
        self.max_steps = 1000
        
        # ìƒíƒœ ê³µê°„ ì°¨ì›: ê° ì¢…ëª©ë³„ (ê°€ê²© ë³€í™”ìœ¨, RSI, ì´ë™í‰ê· , ê±°ë˜ëŸ‰) + ë³´ìœ  í˜„ê¸ˆ ë¹„ìœ¨ + í¬ì§€ì…˜
        self.observation_space_size = len(stocks) * 4 + 1 + len(stocks)
        # ì•¡ì…˜ ê³µê°„: ê° ì¢…ëª©ì— ëŒ€í•´ (ë§¤ìˆ˜, ë³´ìœ , ë§¤ë„)
        self.action_space_size = 3 ** len(stocks)
        
    def reset(self):
        """í™˜ê²½ ë¦¬ì…‹"""
        self.current_balance = self.initial_balance
        self.positions = {stock: 0 for stock in self.stocks}
        self.step_count = 0
        
        # í˜„ì¬ ê°€ê²© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self._update_market_data()
        return self._get_state()
    
    def _update_market_data(self):
        """ì‹œì¥ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        for stock in self.stocks:
            try:
                price_data = self.kis_api.get_stock_price(stock)
                if price_data and price_data.get('rt_cd') == '0':
                    current_price = int(price_data['output']['stck_prpr'])
                    self.prices[stock] = current_price
                    self.price_history[stock].append(current_price)
                    
                    # ìµœê·¼ 30ê°œ ê°€ê²©ë§Œ ìœ ì§€
                    if len(self.price_history[stock]) > 30:
                        self.price_history[stock] = self.price_history[stock][-30:]
                else:
                    logger.warning(f"{stock} ê°€ê²© ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨")
            except Exception as e:
                logger.error(f"{stock} ê°€ê²© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _calculate_technical_indicators(self, stock: str) -> Dict[str, float]:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        prices = self.price_history[stock]
        if len(prices) < 10:
            return {'price_change': 0, 'rsi': 50, 'ma_ratio': 1, 'volume_ma': 1}
        
        # ê°€ê²© ë³€í™”ìœ¨
        price_change = (prices[-1] - prices[-2]) / prices[-2] if len(prices) >= 2 else 0
        
        # RSI ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        gains = []
        losses = []
        for i in range(1, min(len(prices), 15)):
            change = prices[i] - prices[i-1]
            if change > 0:
                gains.append(change)
                losses.append(0)
            else:
                gains.append(0)
                losses.append(abs(change))
        
        avg_gain = np.mean(gains) if gains else 0
        avg_loss = np.mean(losses) if losses else 0.001  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        
        # ì´ë™í‰ê·  ë¹„ìœ¨
        ma_5 = np.mean(prices[-5:]) if len(prices) >= 5 else prices[-1]
        ma_ratio = prices[-1] / ma_5
        
        return {
            'price_change': price_change,
            'rsi': rsi,
            'ma_ratio': ma_ratio,
            'volume_ma': 1.0  # ê±°ë˜ëŸ‰ ì •ë³´ëŠ” ì¶”í›„ ì¶”ê°€
        }
    
    def _get_state(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ë²¡í„° ìƒì„±"""
        state = []
        
        # ê° ì¢…ëª©ë³„ ê¸°ìˆ ì  ì§€í‘œ
        for stock in self.stocks:
            indicators = self._calculate_technical_indicators(stock)
            state.extend([
                indicators['price_change'],
                indicators['rsi'] / 100,  # 0-1 ì •ê·œí™”
                indicators['ma_ratio'] - 1,  # ì¤‘ì‹¬í™”
                indicators['volume_ma']
            ])
        
        # í˜„ê¸ˆ ë¹„ìœ¨
        total_value = self._calculate_portfolio_value()
        cash_ratio = self.current_balance / total_value if total_value > 0 else 1
        state.append(cash_ratio)
        
        # ê° ì¢…ëª©ë³„ í¬ì§€ì…˜ ë¹„ìœ¨
        for stock in self.stocks:
            position_value = self.positions[stock] * self.prices[stock]
            position_ratio = position_value / total_value if total_value > 0 else 0
            state.append(position_ratio)
        
        return np.array(state, dtype=np.float32)
    
    def _calculate_portfolio_value(self) -> float:
        """í¬íŠ¸í´ë¦¬ì˜¤ ì´ ê°€ì¹˜ ê³„ì‚°"""
        total_value = self.current_balance
        for stock in self.stocks:
            total_value += self.positions[stock] * self.prices[stock]
        return total_value
    
    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """ì•¡ì…˜ ì‹¤í–‰"""
        self.step_count += 1
        
        # ì•¡ì…˜ì„ ê° ì¢…ëª©ë³„ í–‰ë™ìœ¼ë¡œ ë¶„í•´
        actions = self._decode_action(action)
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ì´ì „ ê°€ì¹˜
        prev_value = self._calculate_portfolio_value()
        
        # ì‹œì¥ ë°ì´í„° ì—…ë°ì´íŠ¸
        self._update_market_data()
        
        # ê° ì¢…ëª©ì— ëŒ€í•´ ì•¡ì…˜ ì‹¤í–‰
        for i, stock in enumerate(self.stocks):
            stock_action = actions[i]  # 0: ë§¤ë„, 1: ë³´ìœ , 2: ë§¤ìˆ˜
            
            if stock_action == 2:  # ë§¤ìˆ˜
                self._execute_buy(stock)
            elif stock_action == 0:  # ë§¤ë„
                self._execute_sell(stock)
            # 1: ë³´ìœ  (ì•„ë¬´ ì‘ì—… ì•ˆí•¨)
        
        # ìƒˆë¡œìš´ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜
        new_value = self._calculate_portfolio_value()
        
        # ë³´ìƒ ê³„ì‚° (ì¼ì¼ ìˆ˜ìµë¥ )
        reward = (new_value - prev_value) / prev_value if prev_value > 0 else 0
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
        done = (self.step_count >= self.max_steps) or (new_value < self.initial_balance * 0.5)
        
        info = {
            'portfolio_value': new_value,
            'cash': self.current_balance,
            'positions': self.positions.copy(),
            'step': self.step_count
        }
        
        return self._get_state(), reward, done, info
    
    def _decode_action(self, action: int) -> List[int]:
        """í†µí•© ì•¡ì…˜ì„ ê° ì¢…ëª©ë³„ ì•¡ì…˜ìœ¼ë¡œ ë¶„í•´"""
        actions = []
        for i in range(len(self.stocks)):
            actions.append(action % 3)
            action //= 3
        return actions
    
    def _execute_buy(self, stock: str):
        """ë§¤ìˆ˜ ì‹¤í–‰ (í¬íŠ¸í´ë¦¬ì˜¤ì˜ 10% ê¸ˆì•¡ìœ¼ë¡œ)"""
        try:
            total_value = self._calculate_portfolio_value()
            buy_amount = total_value * 0.1  # 10% ë§¤ìˆ˜
            
            if buy_amount > self.current_balance:
                buy_amount = self.current_balance * 0.9  # í˜„ê¸ˆì˜ 90%
            
            if buy_amount > self.prices[stock]:  # ìµœì†Œ 1ì£¼ëŠ” ì‚´ ìˆ˜ ìˆëŠ” ê¸ˆì•¡
                quantity = int(buy_amount / self.prices[stock])
                cost = quantity * self.prices[stock]
                
                if cost <= self.current_balance:
                    self.positions[stock] += quantity
                    self.current_balance -= cost
                    logger.info(f"ë§¤ìˆ˜: {stock} {quantity}ì£¼ @ {self.prices[stock]}ì›")
        
        except Exception as e:
            logger.error(f"ë§¤ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ({stock}): {e}")
    
    def _execute_sell(self, stock: str):
        """ë§¤ë„ ì‹¤í–‰ (ë³´ìœ  ìˆ˜ëŸ‰ì˜ 50%)"""
        try:
            if self.positions[stock] > 0:
                sell_quantity = max(1, self.positions[stock] // 2)  # ìµœì†Œ 1ì£¼
                if sell_quantity <= self.positions[stock]:
                    revenue = sell_quantity * self.prices[stock]
                    self.positions[stock] -= sell_quantity
                    self.current_balance += revenue
                    logger.info(f"ë§¤ë„: {stock} {sell_quantity}ì£¼ @ {self.prices[stock]}ì›")
        
        except Exception as e:
            logger.error(f"ë§¤ë„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ({stock}): {e}")

class DQNAgent:
    """
    Deep Q-Network ì—ì´ì „íŠ¸
    """
    
    def __init__(self, state_size: int, action_size: int, learning_rate: float = 0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.epsilon = 1.0  # íƒí—˜ìœ¨
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.memory = []
        self.memory_size = 10000
        self.batch_size = 32
        self.gamma = 0.95  # í• ì¸ë¥ 
        
        # ì‹ ê²½ë§ ëª¨ë¸ êµ¬ì¶•
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()
    
    def _build_model(self):
        """DQN ëª¨ë¸ êµ¬ì¶•"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(128, activation='relu', input_shape=(self.state_size,)),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='linear')
        ])
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss='mse'
        )
        return model
    
    def update_target_model(self):
        """íƒ€ê²Ÿ ëª¨ë¸ ì—…ë°ì´íŠ¸"""
        self.target_model.set_weights(self.model.get_weights())
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))
        if len(self.memory) > self.memory_size:
            self.memory.pop(0)
    
    def act(self, state, training=True):
        """ì•¡ì…˜ ì„ íƒ (epsilon-greedy)"""
        if training and np.random.random() <= self.epsilon:
            return np.random.choice(self.action_size)
        
        q_values = self.model.predict(state.reshape(1, -1), verbose=0)
        return np.argmax(q_values[0])
    
    def replay(self):
        """ê²½í—˜ ì¬ìƒ í•™ìŠµ"""
        if len(self.memory) < self.batch_size:
            return
        
        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)
        states = np.array([self.memory[i][0] for i in batch])
        actions = np.array([self.memory[i][1] for i in batch])
        rewards = np.array([self.memory[i][2] for i in batch])
        next_states = np.array([self.memory[i][3] for i in batch])
        dones = np.array([self.memory[i][4] for i in batch])
        
        current_q_values = self.model.predict(states, verbose=0)
        next_q_values = self.target_model.predict(next_states, verbose=0)
        
        targets = current_q_values.copy()
        
        for i in range(self.batch_size):
            if dones[i]:
                targets[i][actions[i]] = rewards[i]
            else:
                targets[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])
        
        self.model.fit(states, targets, epochs=1, verbose=0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_model(self, filepath: str):
        """ëª¨ë¸ ì €ì¥"""
        self.model.save(filepath)
        logger.info(f"ëª¨ë¸ ì €ì¥: {filepath}")
    
    def load_model(self, filepath: str):
        """ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(filepath):
            self.model = tf.keras.models.load_model(filepath)
            self.update_target_model()
            logger.info(f"ëª¨ë¸ ë¡œë“œ: {filepath}")
            return True
        return False

class AITradingSystem:
    """
    AI ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ë©”ì¸ í´ë˜ìŠ¤
    """
    
    def __init__(self, mode='demo'):
        self.mode = mode
        self.kis_api = None
        self.trading_env = None
        self.agent = None
        self.is_running = False
        self.stocks = ['005930', '000660', '051910']  # ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, LGí™”í•™
        
        # ë¡œê·¸ íŒŒì¼ ì„¤ì •
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        self.performance_log = log_dir / f"performance_{mode}.json"
        self.model_path = f"models/dqn_model_{mode}.keras"
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
        Path("models").mkdir(exist_ok=True)
        
    def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # API ì´ˆê¸°í™”
            account_info = Config.get_account_info(self.mode)
            self.kis_api = KisAPI(
                account_info['appkey'],
                account_info['appsecret'], 
                account_info['account'],
                is_real=(self.mode == 'real')
            )
            
            # í† í° ë°œê¸‰
            if not self.kis_api.get_access_token():
                raise Exception("API í† í° ë°œê¸‰ ì‹¤íŒ¨")
            
            # íŠ¸ë ˆì´ë”© í™˜ê²½ ì´ˆê¸°í™”
            self.trading_env = TradingEnvironment(self.kis_api, self.stocks)
            
            # DQN ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
            self.agent = DQNAgent(
                state_size=self.trading_env.observation_space_size,
                action_size=self.trading_env.action_space_size
            )
            
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ìˆë‹¤ë©´)
            self.agent.load_model(self.model_path)
            
            logger.info(f"AI ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ ({self.mode} ëª¨ë“œ)")
            return True
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            return False
    
    def train_agent(self, episodes=1000):
        """ì—ì´ì „íŠ¸ í›ˆë ¨"""
        logger.info(f"DQN ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œì‘ (ì—í”¼ì†Œë“œ: {episodes})")
        
        scores = []
        best_score = float('-inf')
        
        for episode in range(episodes):
            state = self.trading_env.reset()
            total_reward = 0
            step = 0
            
            while True:
                action = self.agent.act(state, training=True)
                next_state, reward, done, info = self.trading_env.step(action)
                
                self.agent.remember(state, action, reward, next_state, done)
                state = next_state
                total_reward += reward
                step += 1
                
                if done:
                    break
                
                # ì£¼ê¸°ì  í•™ìŠµ
                if step % 10 == 0:
                    self.agent.replay()
            
            scores.append(total_reward)
            
            # íƒ€ê²Ÿ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if episode % 10 == 0:
                self.agent.update_target_model()
            
            # ì„±ëŠ¥ ë¡œê¹…
            if episode % 100 == 0:
                avg_score = np.mean(scores[-100:])
                logger.info(f"ì—í”¼ì†Œë“œ {episode}, í‰ê·  ì ìˆ˜: {avg_score:.4f}, ì—¡ì‹¤ë¡ : {self.agent.epsilon:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if avg_score > best_score:
                    best_score = avg_score
                    self.agent.save_model(self.model_path)
        
        logger.info("í›ˆë ¨ ì™„ë£Œ")
        return scores
    
    def run_live_trading(self):
        """ì‹¤ì‹œê°„ íŠ¸ë ˆì´ë”© ì‹¤í–‰"""
        if not self.initialize():
            return
        
        logger.info("ì‹¤ì‹œê°„ AI ìë™ë§¤ë§¤ ì‹œì‘")
        self.is_running = True
        
        try:
            while self.is_running:
                # í† í° ê°±ì‹  í™•ì¸
                self.kis_api.refresh_token_if_needed()
                
                # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                self.trading_env._update_market_data()
                state = self.trading_env._get_state()
                
                # AI ëª¨ë¸ë¡œ ì•¡ì…˜ ê²°ì • (íƒí—˜ ì—†ì´)
                action = self.agent.act(state, training=False)
                
                # ì‹¤ì œ ê±°ë˜ ì‹¤í–‰ì€ ë³„ë„ ë¡œì§ í•„ìš”
                self._execute_real_trading(action)
                
                # ì„±ëŠ¥ ê¸°ë¡
                self._log_performance()
                
                # 30ë¶„ ëŒ€ê¸° (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì¡°ì • ê°€ëŠ¥)
                time.sleep(30 * 60)
                
        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•œ íŠ¸ë ˆì´ë”© ì¤‘ë‹¨")
        except Exception as e:
            logger.error(f"íŠ¸ë ˆì´ë”© ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
        finally:
            self.stop()
    
    def _execute_real_trading(self, action: int):
        """ì‹¤ì œ ê±°ë˜ ì‹¤í–‰ (ë§¤ìš° ë³´ìˆ˜ì ìœ¼ë¡œ)"""
        try:
            # ì‹¤ì œ ì”ê³  ì¡°íšŒ
            balance = self.kis_api.get_balance()
            if not balance:
                logger.warning("ì”ê³  ì¡°íšŒ ì‹¤íŒ¨")
                return
            
            # ë³´ìœ  ì¢…ëª© í™•ì¸
            holdings = self.kis_api.get_holding_stocks()
            available_cash = self.kis_api.get_available_cash()
            
            logger.info(f"í˜„ì¬ ë³´ìœ  í˜„ê¸ˆ: {available_cash:,}ì›")
            logger.info(f"ë³´ìœ  ì¢…ëª© ìˆ˜: {len(holdings)}")
            
            # ì—¬ê¸°ì„œ ì‹¤ì œ ë§¤ë§¤ ë¡œì§ êµ¬í˜„
            # ì•ˆì „ì„ ìœ„í•´ ì‘ì€ ê¸ˆì•¡ìœ¼ë¡œë§Œ í…ŒìŠ¤íŠ¸
            
        except Exception as e:
            logger.error(f"ì‹¤ì œ ê±°ë˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _log_performance(self):
        """ì„±ëŠ¥ ë¡œê¹…"""
        try:
            performance_data = {
                'timestamp': datetime.now().isoformat(),
                'balance': self.kis_api.get_available_cash(),
                'holdings': self.kis_api.get_holding_stocks(),
                'mode': self.mode
            }
            
            # íŒŒì¼ì— ì¶”ê°€
            logs = []
            if self.performance_log.exists():
                with open(self.performance_log, 'r') as f:
                    logs = json.load(f)
            
            logs.append(performance_data)
            
            # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            if len(logs) > 1000:
                logs = logs[-1000:]
            
            with open(self.performance_log, 'w') as f:
                json.dump(logs, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë¡œê¹… ì¤‘ ì˜¤ë¥˜: {e}")
    
    def stop(self):
        """ì‹œìŠ¤í…œ ì¤‘ë‹¨"""
        self.is_running = False
        logger.info("AI ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ì¤‘ë‹¨")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤– AI ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ëª¨ë“œ ì„ íƒ
    while True:
        mode = input("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš” (demo/real/train): ").strip().lower()
        if mode in ['demo', 'real', 'train']:
            break
        print("ì˜¬ë°”ë¥¸ ëª¨ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: demo, real, train")
    
    system = AITradingSystem(mode='demo' if mode in ['demo', 'train'] else 'real')
    
    try:
        if mode == 'train':
            if system.initialize():
                print("ğŸ§  AI ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                episodes = int(input("í›ˆë ¨ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 1000): ") or "1000")
                system.train_agent(episodes)
        else:
            print(f"ğŸš€ ì‹¤ì‹œê°„ ìë™ë§¤ë§¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ ({mode} ëª¨ë“œ)")
            print("ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”")
            system.run_live_trading()
    
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()