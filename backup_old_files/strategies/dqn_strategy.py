"""
ê°•í™”í•™ìŠµ(DQN) ê¸°ë°˜ ìë™ ì „ëµ í•™ìŠµ
ì°¸ì¡°: https://twentytwentyone.tistory.com/1873

[ì£¼ìš” íŠ¹ì§•]
- Deep Q-Networkë¥¼ ì‚¬ìš©í•œ ìë™ ì „ëµ í•™ìŠµ
- ì‹œì¥ ìƒíƒœë¥¼ í•™ìŠµí•˜ì—¬ ìµœì ì˜ í–‰ë™ ì„ íƒ
- ê²½í—˜ ì¬ìƒ(Experience Replay)ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ
- ìë™ìœ¼ë¡œ íˆ¬ì ì „ëµ ê°œì„ 
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from datetime import datetime
from typing import Dict, List, Tuple, Optional

from indicators.technical import TechnicalIndicators


class DQN(nn.Module):
    """Deep Q-Network ëª¨ë¸"""
    
    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, action_size)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x


class DQNAgent:
    """DQN ì—ì´ì „íŠ¸"""
    
    def __init__(self, state_size: int, action_size: int = 3):  # ë§¤ìˆ˜, ë§¤ë„, í™€ë“œ
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=10000)
        self.epsilon = 1.0  # íƒí—˜ í™•ë¥ 
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.95  # í• ì¸ ê³„ìˆ˜
        self.learning_rate = 0.001
        
        # ì‹ ê²½ë§
        self.q_network = DQN(state_size, action_size)
        self.target_network = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        
        self.update_target_network()
        
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
        
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))
        
    def act(self, state):
        """í–‰ë™ ì„ íƒ (Îµ-greedy)"""
        if random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        q_values = self.q_network(state_tensor)
        return np.argmax(q_values.detach().numpy())
        
    def replay(self, batch_size: int = 32):
        """ê²½í—˜ ì¬ìƒìœ¼ë¡œ í•™ìŠµ"""
        if len(self.memory) < batch_size:
            return
            
        batch = random.sample(self.memory, batch_size)
        states = torch.FloatTensor([e[0] for e in batch])
        actions = torch.LongTensor([e[1] for e in batch])
        rewards = torch.FloatTensor([e[2] for e in batch])
        next_states = torch.FloatTensor([e[3] for e in batch])
        dones = torch.FloatTensor([e[4] for e in batch])
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))
        
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # ì—¡ì‹¤ë¡  ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


class DQNStrategy:
    """ê°•í™”í•™ìŠµ ê¸°ë°˜ ìë™ ì „ëµ"""
    
    def __init__(self, api, config: Dict = None):
        self.api = api
        self.indicators = TechnicalIndicators()
        
        # ê¸°ë³¸ ì„¤ì •
        default_config = {
            'state_size': 20,           # ìƒíƒœ ë²¡í„° í¬ê¸°
            'lookback_period': 20,      # ê³¼ê±° ë°ì´í„° ê¸°ê°„
            'initial_balance': 10000000,  # ì´ˆê¸° ìê¸ˆ
            'position_size': 0.2,       # í¬ì§€ì…˜ í¬ê¸°
            'min_price': 5000,
            'max_price': 500000,
            'max_stocks': 5,
            'learning_episodes': 100,   # í•™ìŠµ ì—í”¼ì†Œë“œ
            'train_interval': 1000,     # í•™ìŠµ ì£¼ê¸°
        }
        
        self.config = {**default_config, **(config or {})}
        
        # DQN ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent = DQNAgent(self.config['state_size'])
        
        # ê±°ë˜ ê¸°ë¡
        self.trade_history = []
        self.portfolio = {}
        self.balance = self.config['initial_balance']
        self.total_trades = 0
        self.winning_trades = 0
        
        # í•™ìŠµ ë°ì´í„°
        self.state_buffer = deque(maxlen=self.config['lookback_period'])
        
    def extract_features(self, stock_data: List[Dict]) -> np.ndarray:
        """ì‹œì¥ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        if len(stock_data) < self.config['lookback_period']:
            return None
            
        # ê°€ê²© ë°ì´í„°
        prices = [float(d.get('stck_clpr', 0)) for d in stock_data]
        volumes = [int(d.get('acml_vol', 0)) for d in stock_data]
        
        if not prices or len(prices) < self.config['lookback_period']:
            return None
            
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        analysis = self.indicators.analyze_stock(prices, volumes)
        
        # íŠ¹ì§• ë²¡í„° êµ¬ì„±
        features = []
        
        # ê°€ê²© ë³€í™”ìœ¨
        returns = [(prices[i] - prices[i-1]) / prices[i-1] if prices[i-1] > 0 else 0 
                  for i in range(1, len(prices))]
        features.extend(returns[-5:])  # ìµœê·¼ 5ì¼ ìˆ˜ìµë¥ 
        
        # ê¸°ìˆ ì  ì§€í‘œ
        features.append(analysis.get('rsi', 50) / 100)  # RSI ì •ê·œí™”
        features.append(1.0 if analysis.get('price_above_ma5', False) else 0.0)
        features.append(1.0 if analysis.get('price_above_ma20', False) else 0.0)
        features.append(min(analysis.get('volume_ratio', 1.0) / 5, 1.0))  # ê±°ë˜ëŸ‰ ë¹„ìœ¨
        
        # ë³¼ë¦°ì € ë°´ë“œ ìœ„ì¹˜
        current_price = prices[-1]
        ma20 = np.mean(prices[-20:])
        std20 = np.std(prices[-20:])
        bb_position = (current_price - ma20) / (2 * std20 + 1e-10)
        features.append(np.clip(bb_position, -1, 1))
        
        # ëª¨ë©˜í…€
        momentum_5 = (prices[-1] - prices[-6]) / prices[-6] if len(prices) > 5 and prices[-6] > 0 else 0
        momentum_20 = (prices[-1] - prices[-21]) / prices[-21] if len(prices) > 20 and prices[-21] > 0 else 0
        features.append(np.clip(momentum_5, -0.2, 0.2))
        features.append(np.clip(momentum_20, -0.3, 0.3))
        
        # íŒ¨ë”©ìœ¼ë¡œ ê³ ì • í¬ê¸° ë§ì¶”ê¸°
        while len(features) < self.config['state_size']:
            features.append(0.0)
            
        return np.array(features[:self.config['state_size']])
    
    def calculate_reward(self, action: int, price_change: float, 
                        holding_position: bool) -> float:
        """ë³´ìƒ ê³„ì‚°"""
        reward = 0.0
        
        if action == 0:  # ë§¤ìˆ˜
            if not holding_position and price_change > 0:
                reward = price_change * 10  # ì¢‹ì€ ë§¤ìˆ˜ íƒ€ì´ë°
            elif not holding_position and price_change < 0:
                reward = price_change * 5  # ë‚˜ìœ ë§¤ìˆ˜ íƒ€ì´ë°
            elif holding_position:
                reward = -0.1  # ì´ë¯¸ ë³´ìœ  ì¤‘ì¸ë° ë§¤ìˆ˜ ì‹œë„
                
        elif action == 1:  # ë§¤ë„
            if holding_position and price_change < 0:
                reward = -price_change * 10  # ì¢‹ì€ ë§¤ë„ íƒ€ì´ë° (ì†ì‹¤ íšŒí”¼)
            elif holding_position and price_change > 0:
                reward = price_change * 5  # ìˆ˜ìµ ì‹¤í˜„
            elif not holding_position:
                reward = -0.1  # ë³´ìœ í•˜ì§€ ì•Šì€ë° ë§¤ë„ ì‹œë„
                
        else:  # í™€ë“œ
            reward = -0.01  # ì‘ì€ í˜ë„í‹° (ê±°ë˜ ì´‰ì§„)
            
        return reward
    
    def get_action_name(self, action: int) -> str:
        """í–‰ë™ ì´ë¦„ ë°˜í™˜"""
        actions = {0: "BUY", 1: "SELL", 2: "HOLD"}
        return actions.get(action, "UNKNOWN")
    
    def train_on_historical_data(self, stock_code: str, days: int = 100):
        """ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµ"""
        print(f"\nğŸ§  {stock_code} ì¢…ëª©ìœ¼ë¡œ DQN í•™ìŠµ ì‹œì‘...")
        
        # ê³¼ê±° ë°ì´í„° ì¡°íšŒ
        daily_data = self.api.get_daily_price(stock_code, days=days)
        if not daily_data or daily_data.get('rt_cd') != '0':
            print("âŒ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨")
            return
            
        stock_data = daily_data.get('output', [])
        if len(stock_data) < self.config['lookback_period'] + 1:
            print("âŒ ë°ì´í„° ë¶€ì¡±")
            return
            
        # ì—í”¼ì†Œë“œë³„ í•™ìŠµ
        for episode in range(self.config['learning_episodes']):
            episode_reward = 0
            holding = False
            
            for i in range(self.config['lookback_period'], len(stock_data) - 1):
                # í˜„ì¬ ìƒíƒœ
                state = self.extract_features(stock_data[:i+1])
                if state is None:
                    continue
                    
                # í–‰ë™ ì„ íƒ
                action = self.agent.act(state)
                
                # ë‹¤ìŒ ê°€ê²©
                current_price = float(stock_data[i]['stck_clpr'])
                next_price = float(stock_data[i+1]['stck_clpr'])
                price_change = (next_price - current_price) / current_price
                
                # ë³´ìƒ ê³„ì‚°
                reward = self.calculate_reward(action, price_change, holding)
                episode_reward += reward
                
                # í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
                if action == 0 and not holding:  # ë§¤ìˆ˜
                    holding = True
                elif action == 1 and holding:  # ë§¤ë„
                    holding = False
                
                # ë‹¤ìŒ ìƒíƒœ
                next_state = self.extract_features(stock_data[:i+2])
                done = (i == len(stock_data) - 2)
                
                if next_state is not None:
                    # ê²½í—˜ ì €ì¥
                    self.agent.remember(state, action, reward, next_state, done)
                    
                # í•™ìŠµ
                if len(self.agent.memory) > 32:
                    self.agent.replay(32)
                    
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if episode % 10 == 0:
                self.agent.update_target_network()
                
            if episode % 20 == 0:
                print(f"   ì—í”¼ì†Œë“œ {episode}: ì´ ë³´ìƒ = {episode_reward:.2f}, Îµ = {self.agent.epsilon:.3f}")
                
        print("âœ… DQN í•™ìŠµ ì™„ë£Œ!")
    
    def analyze_stock(self, stock_code: str) -> Dict:
        """DQNìœ¼ë¡œ ì¢…ëª© ë¶„ì„"""
        # ìµœê·¼ ë°ì´í„° ì¡°íšŒ
        daily_data = self.api.get_daily_price(stock_code)
        if not daily_data or daily_data.get('rt_cd') != '0':
            return {'signal': 'HOLD', 'confidence': 0, 'reason': 'ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨'}
            
        stock_data = daily_data.get('output', [])
        if len(stock_data) < self.config['lookback_period']:
            return {'signal': 'HOLD', 'confidence': 0, 'reason': 'ë°ì´í„° ë¶€ì¡±'}
            
        # í˜„ì¬ ìƒíƒœ ì¶”ì¶œ
        state = self.extract_features(stock_data[:self.config['lookback_period']+1])
        if state is None:
            return {'signal': 'HOLD', 'confidence': 0, 'reason': 'íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨'}
            
        # Qê°’ ê³„ì‚°
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = self.agent.q_network(state_tensor).squeeze().numpy()
            
        # ìµœì  í–‰ë™ ì„ íƒ
        action = np.argmax(q_values)
        confidence = abs(q_values[action] - np.mean(q_values)) * 100
        confidence = min(100, max(0, confidence))
        
        # ì‹ í˜¸ ìƒì„±
        signal = self.get_action_name(action)
        
        # ì´ìœ  ìƒì„±
        if action == 0:
            reason = f"DQN ëª¨ë¸ì´ ë§¤ìˆ˜ ì‹ í˜¸ ê°ì§€ (Qê°’: {q_values[0]:.2f})"
        elif action == 1:
            reason = f"DQN ëª¨ë¸ì´ ë§¤ë„ ì‹ í˜¸ ê°ì§€ (Qê°’: {q_values[1]:.2f})"
        else:
            reason = f"DQN ëª¨ë¸ì´ ê´€ë§ ê¶Œê³  (Qê°’: {q_values[2]:.2f})"
            
        return {
            'signal': signal,
            'confidence': int(confidence),
            'reason': reason,
            'q_values': q_values.tolist(),
            'epsilon': self.agent.epsilon
        }
    
    def run_once(self) -> Dict:
        """ì „ëµ 1íšŒ ì‹¤í–‰"""
        result = {
            'timestamp': datetime.now().isoformat(),
            'strategy': 'dqn',
            'buys': [],
            'sells': [],
            'analysis': [],
            'errors': []
        }
        
        # ë³´ìœ  ì¢…ëª© í™•ì¸
        holdings = self.api.get_holding_stocks()
        holding_codes = [h['stock_code'] for h in holdings] if holdings else []
        
        # ê±°ë˜ëŸ‰ ìƒìœ„ ì¢…ëª© ì¡°íšŒ
        volume_stocks = []
        for market in ['J', 'Q']:  # KOSPI, KOSDAQ
            volume_data = self.api.get_volume_rank(market)
            if volume_data and volume_data.get('rt_cd') == '0':
                stocks = volume_data.get('output', [])[:10]
                for stock in stocks:
                    try:
                        code = stock.get('mksc_shrn_iscd', '')
                        name = stock.get('hts_kor_isnm', '')
                        price = int(stock.get('stck_prpr', 0))
                        
                        if code and self.config['min_price'] <= price <= self.config['max_price']:
                            volume_stocks.append({
                                'code': code,
                                'name': name,
                                'price': price
                            })
                    except (ValueError, TypeError):
                        continue
        
        # DQNìœ¼ë¡œ ê° ì¢…ëª© ë¶„ì„
        analyzed_count = 0
        for stock in volume_stocks[:20]:  # ìƒìœ„ 20ê°œë§Œ
            if analyzed_count >= 5:  # ìµœëŒ€ 5ê°œ ë¶„ì„
                break
                
            code = stock['code']
            name = stock['name']
            
            # ì´ë¯¸ ë³´ìœ  ì¤‘ì´ë©´ ìŠ¤í‚µ
            if code in holding_codes:
                continue
                
            print(f"\nğŸ¤– DQN ë¶„ì„: {name} ({code})")
            
            # DQN ë¶„ì„
            analysis = self.analyze_stock(code)
            result['analysis'].append({
                'code': code,
                'name': name,
                **analysis
            })
            
            # ë§¤ìˆ˜ ì‹ í˜¸ì´ê³  ì‹ ë¢°ë„ê°€ ë†’ìœ¼ë©´ ë§¤ìˆ˜
            if analysis['signal'] == 'BUY' and analysis['confidence'] >= 60:
                # ë§¤ìˆ˜ ìˆ˜ëŸ‰ ê³„ì‚°
                available_cash = self.api.get_available_cash()
                max_investment = int(available_cash * self.config['position_size'])
                quantity = max_investment // stock['price']
                
                if quantity > 0:
                    print(f"   â†’ ë§¤ìˆ˜ ì‹¤í–‰: {quantity}ì£¼")
                    result['buys'].append({
                        'code': code,
                        'name': name,
                        'quantity': quantity,
                        'price': stock['price'],
                        'confidence': analysis['confidence'],
                        'reason': analysis['reason']
                    })
                
            analyzed_count += 1
        
        # ë³´ìœ  ì¢…ëª© ë§¤ë„ ê²€í† 
        for holding in holdings:
            code = holding['stock_code']
            name = holding['stock_name']
            
            analysis = self.analyze_stock(code)
            
            if analysis['signal'] == 'SELL' and analysis['confidence'] >= 60:
                print(f"\nğŸ’° ë§¤ë„ ì‹ í˜¸: {name} ({code})")
                result['sells'].append({
                    'code': code,
                    'name': name,
                    'quantity': holding['quantity'],
                    'confidence': analysis['confidence'],
                    'reason': analysis['reason']
                })
        
        # í•™ìŠµ (ì£¼ê¸°ì ìœ¼ë¡œ)
        self.total_trades += 1
        if self.total_trades % self.config['train_interval'] == 0:
            print("\nğŸ§  ëª¨ë¸ ì¬í•™ìŠµ ì¤‘...")
            # ìµœê·¼ ê±°ë˜ê°€ í™œë°œí•œ ì¢…ëª©ìœ¼ë¡œ ì¬í•™ìŠµ
            if volume_stocks:
                self.train_on_historical_data(volume_stocks[0]['code'])
        
        return result
    
    def get_status(self) -> Dict:
        """í˜„ì¬ ìƒíƒœ ì¡°íšŒ"""
        return {
            'strategy': 'dqn',
            'epsilon': self.agent.epsilon,
            'memory_size': len(self.agent.memory),
            'total_trades': self.total_trades,
            'winning_trades': self.winning_trades,
            'win_rate': (self.winning_trades / self.total_trades * 100) if self.total_trades > 0 else 0,
            'balance': self.balance,
            'portfolio': self.portfolio,
            'config': self.config
        }